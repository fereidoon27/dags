# Understanding Airflow Task Logs & Timestamp Issues

## üìù Airflow Task Log Structure Explained

When you view a task log in the Airflow UI, you see three distinct sections:

### 1. **Pre Task Execution Logs** (‚ñ∂ Pre task execution logs)

**What it is:** Automatically generated by Airflow **before** your task code runs

**What it contains:**
- Task dependency checks
- Resource allocation
- Task queue placement
- Executor submission
- Environment setup

**Who generates it:** Airflow Scheduler and Executor (not your DAG code)

**Example:**
```
[2025-10-28, 15:00:25 +0330] {taskinstance.py:1123} INFO - Dependencies all met for <TaskInstance: ...>
[2025-10-28, 15:00:25 +0330] {taskinstance.py:1324} INFO - Starting attempt 1 of 1
[2025-10-28, 15:00:25 +0330] {taskinstance.py:1345} INFO - Executing <Task(PythonOperator): check_celery_1_airflow_worker> on 2025-10-28 11:30:22.050978+00:00
```

### 2. **Main Task Body** (the middle section, no collapsible header)

**What it is:** Output from **your task's code** (the actual DAG logic)

**What it contains:**
- All print() statements from your functions
- Custom logging from your code
- Any output your task produces
- Error messages from your code

**Who generates it:** Your DAG code (the functions you write)

**Example:**
```
[2025-10-28, 15:00:31 +0330] {logging_mixin.py:188} INFO - üîç Checking airflow-worker remotely on celery-1 (10.101.20.199)
[2025-10-28, 15:00:32 +0330] {logging_mixin.py:188} INFO - üìã FETCHING DIAGNOSTIC LOGS FOR FAILED SERVICE
[2025-10-28, 15:00:32 +0330] {logging_mixin.py:188} INFO - ‚úÖ airflow-worker on celery-1 (10.101.20.199) is ACTIVE
```

### 3. **Post Task Execution Logs** (‚ñ∂ Post task execution logs)

**What it is:** Automatically generated by Airflow **after** your task completes

**What it contains:**
- Task completion status
- Return value handling
- XCom push operations
- Cleanup operations
- Execution duration

**Who generates it:** Airflow Worker and Executor (not your DAG code)

**Example:**
```
[2025-10-28, 15:00:35 +0330] {python.py:237} INFO - Done. Returned value was: {'status': 'active', 'hostname': 'celery-1', ...}
[2025-10-28, 15:00:35 +0330] {taskinstance.py:1456} INFO - Marking task as SUCCESS
[2025-10-28, 15:00:35 +0330] {local_task_job_runner.py:234} INFO - Task exited with return code 0
```

---

## üïê Timestamp Issues - "[Invalid date]" Problem

### The Problem You're Seeing

In scheduler logs, you see:
```
Oct 28 15:00:27 haproxy-1 airflow-scheduler-ha1[Invalid date] {scheduler_job_runner.py:480} INFO - DAG ha_service_health_monitor_enhanced has 16/16 running and queued tasks
```

### Why This Happens

The "[Invalid date]" appears because of **mismatched timestamp formats** when using grep to filter logs:

1. **Journalctl output** uses format: `Oct 28 15:00:27 hostname service[pid]: message`
2. **Airflow logs inside** use format: `[2025-10-28, 15:00:27 +0330] {file.py:line} LEVEL - message`
3. **When you grep**, you get both formats mixed together
4. The grep extracts lines that don't have the leading timestamp, so it shows `[Invalid date]`

### The Fix in v2

I changed from:
```bash
# OLD - causes [Invalid date]
journalctl -u airflow-scheduler --since '2 minutes ago' | grep 'task_id' | tail -50
```

To:
```bash
# NEW - preserves proper timestamps
journalctl -u airflow-scheduler --since '2 minutes ago' --no-pager -o short-iso | grep 'task_id' | tail -50
```

**Key changes:**
- Added `-o short-iso` flag to journalctl (uses ISO 8601 timestamps)
- Removed complex grep patterns that were matching partial lines
- Better timestamp preservation

### About UTC vs Local Time

**You asked about time translation:**

> "did you implement mechanism in code to translate utc time to my desire time?"

**Answer: NO - and you shouldn't need to!**

Here's why:

1. **Journalctl automatically shows local time** if your system timezone is set correctly
2. **Airflow logs automatically use local time** if configured properly
3. **Manually translating timestamps in code is error-prone and unnecessary**

### Proper Solution (What You Should Do)

Instead of translating timestamps in the DAG code:

1. **Verify system timezone on all nodes:**
   ```bash
   timedatectl
   # Should show: Time zone: Asia/Tehran (+0330, +0330)
   ```

2. **Verify journalctl uses local time:**
   ```bash
   journalctl -u airflow-scheduler -n 5
   # Timestamps should already be in Tehran time
   ```

3. **If timestamps are still wrong:**
   ```bash
   # Force timezone for journalctl
   export TZ='Asia/Tehran'
   journalctl -u airflow-scheduler -n 5
   ```

4. **In the v2 DAG, I use `-o short-iso` which includes timezone:**
   ```
   2025-10-28T15:00:27+0330  # Includes +0330 timezone offset
   ```

---

## üîç The "execution_date" Deprecation Warning

### Warning You're Seeing

```
AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
```

### Why This Happens

In Airflow 2.2+, `execution_date` was renamed to make the terminology clearer:
- **Old:** `execution_date` (confusing name - not when task executes!)
- **New:** `logical_date` or `data_interval_start` (clearer meaning)

### The Fix in v2

I changed from:
```python
# OLD - triggers deprecation warning
execution_date = context['execution_date']
```

To:
```python
# NEW - uses modern Airflow 2.x approach
# Access via task instance directly (no deprecation warning)
identifiers = {}
ti = context.get('ti')
if ti:
    identifiers['execution_date'] = str(ti.execution_date)  # Direct access, no warning
```

**Or alternatively:**
```python
# Use new property names
logical_date = context.get('logical_date')
data_interval_start = context.get('data_interval_start')
```

---

## üìä Summary of Changes in v2

### 1. Fixed NFS Active/Passive Logic
```python
# CRITICAL FIX: Pass active_nfs as dynamic argument
nfs_tasks.append(check_nfs_service(active_nfs))  # NOT as default parameter!
```

### 2. Changed to 2-Minute Monitoring
```python
schedule='*/2 * * * *',  # Every 2 minutes
```

### 3. Reduced Log Fetch Duration
```python
fetch_journalctl_logs(ip, service, hostname, minutes_back=2)  # Not 60
```

### 4. Enhanced Task-Specific Tracing
```python
def get_task_identifiers(context):
    """Extract unique identifiers for tracing across logs"""
    # Returns: dag_id, task_id, run_id, job_id, external_executor_id, PID, ti_key
```

### 5. Fixed Timestamp Format
```python
# Use short-iso format for consistent timestamps
cmd = f"journalctl -u {service} --since '{minutes_back} minutes ago' --no-pager -o short-iso"
```

### 6. Improved Log Filtering
```python
def fetch_task_specific_logs(identifiers, minutes_back=2):
    """
    Searches logs across ALL Airflow components (schedulers, workers)
    using task-specific identifiers to trace this exact task instance
    """
```

---

## üéØ Task Identifier Tracing (Your Request #3)

### What Identifiers Are Tracked

The v2 DAG now extracts and uses these identifiers:

```python
{
    'dag_id': 'ha_service_health_monitor_enhanced',
    'task_id': 'check_celery_1_airflow_worker',
    'run_id': 'manual__2025-10-28T11:30:22.050978+00:00',
    'execution_date': '2025-10-28 11:30:22.050978+00:00',
    'try_number': 1,
    'job_id': 12345,                    # Internal Airflow job ID
    'external_executor_id': 'abc-123',   # Celery task ID
    'ti_key': 'dag_id__task_id__exec_date',  # Unique TaskInstance key
    'pid': 67890,                        # Process ID on worker
    'map_index': -1                      # For mapped tasks
}
```

### How It's Used

1. **Build grep pattern:**
   ```python
   grep_pattern = 'task_id|run_id|job_id|external_executor_id'
   ```

2. **Search across all components:**
   ```python
   # Searches: Scheduler 1, Scheduler 2, Worker 1, Worker 2
   # Finds: All log entries mentioning this specific task instance
   ```

3. **Display in failed task log:**
   ```
   üìã TASK-SPECIFIC AIRFLOW LOGS (across all components):
   ================================================================================
   üìã Scheduler haproxy-1 logs for this task:
   Oct 28 15:00:27 haproxy-1 airflow-scheduler: Sending task check_celery_1_airflow_worker...
   
   üìã Worker celery-1 logs for this task:
   Oct 28 15:00:31 celery-1 airflow-worker: Executing task check_celery_1_airflow_worker...
   ```

---

## üìå Quick Reference: What's What

| Section | Generated By | Contains | Controlled By |
|---------|-------------|----------|---------------|
| Pre-execution logs | Airflow (automatic) | Dependency checks, task queuing | Airflow core code |
| Main task body | Your DAG code | Your print() statements, logic output | You (DAG author) |
| Post-execution logs | Airflow (automatic) | Status, return values, cleanup | Airflow core code |
| Service logs (journalctl) | System services | Service-specific events | systemd/service |
| Task-specific logs | Airflow components | Logs mentioning this task | Airflow (filtered by DAG) |

---

## ‚úÖ Deployment Checklist for v2

```bash
# 1. Deploy v2 DAG
scp ha_service_health_monitor_enhanced_v2.py rocky@10.101.20.165:/srv/airflow/dags/

# 2. Verify it appears in UI
# Check for: ha_service_health_monitor_enhanced (with v2 tag)

# 3. Trigger manual run and check:
# ‚úì NFS active/passive logic works correctly
# ‚úì No "[Invalid date]" in logs
# ‚úì No execution_date deprecation warnings
# ‚úì Task-specific logs appear in failed tasks
# ‚úì Timestamps show Asia/Tehran (+0330)
```

---

## üéì Key Takeaways

1. **Pre/Post logs are automatic** - you don't control them
2. **Main body is your code** - your print() statements appear here
3. **[Invalid date] was a grep issue** - fixed with `-o short-iso`
4. **Don't translate timestamps in code** - let system handle it
5. **v2 uses task identifiers** - traces task across all components
6. **Active NFS now works correctly** - fixed parameter passing

---

**The v2 DAG addresses all your issues!**
